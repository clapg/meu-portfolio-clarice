{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP37s7n8cjywjErtNiOteiW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clapg/meu-portfolio-clarice/blob/main/EDA_consolidado_Squad8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA - Squad 08"
      ],
      "metadata": {
        "id": "o22wiTR64eRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Leitura das bases"
      ],
      "metadata": {
        "id": "ewew_Wg26Nme"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar dicionário para armazenar DataFrames\n",
        "dataframes = {}\n",
        "caminho_root = '/Volumes/hackathon_2025/default/source/'\n",
        "\n",
        "# base_dados_cadastrais\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}base_dados_cadastrais/')\n",
        "    dataframes['base_dados_cadastrais'] = df\n",
        "    print(f'base_dados_cadastrais:{df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# base_score_bureau_movel\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}base_score_bureau_movel/')\n",
        "    dataframes['base_score_bureau_movel'] = df\n",
        "    print(f'base_score_bureau_movel:{df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# base_score_bureau_movel_full (NOVA)\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}base_score_bureau_movel_full/')\n",
        "    dataframes['base_score_bureau_movel_full'] = df\n",
        "    print(f'base_score_bureau_movel_full:{df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# base_telco\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}base_telco/')\n",
        "    dataframes['base_telco'] = df\n",
        "    print(f'base_telco:{df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# csv de bases_recarga\n",
        "csv_files = [\n",
        "    'BI_DIM_CANAL_AQUISICAO_CREDITO',\n",
        "    'BI_DIM_FORMA_PAGAMENTO',\n",
        "    'BI_DIM_INSTITUICAO',\n",
        "    'BI_DIM_PLANO_PRECO',\n",
        "    'BI_DIM_PLATAFORMA',\n",
        "    'BI_DIM_PROMOCAO_CREDITO',\n",
        "    'BI_DIM_STATUS_PLATAFORMA',\n",
        "    'BI_DIM_TECNOLOGIA',\n",
        "    'BI_DIM_TIPO_CREDITO',\n",
        "    'BI_DIM_TIPO_INSERCAO',\n",
        "    'BI_DIM_TIPO_RECARGA'\n",
        "]\n",
        "\n",
        "for csv_file in csv_files:\n",
        "    try:\n",
        "        df = spark.read.csv(f'{caminho_root}bases_recarga/{csv_file}.csv', header=True, inferSchema=True)\n",
        "        dataframes[csv_file] = df\n",
        "        print(f'{csv_file}: {df.count():,} linhas')\n",
        "    except Exception as e:\n",
        "        print(f'{csv_file}: {str(e)[:50]}')\n",
        "\n",
        "# BI_DIM_TIPO_FATURAMENTO\n",
        "try:\n",
        "    df = spark.read.csv(f'{caminho_root}book_atraso/BI_DIM_TIPO_FATURAMENTO.csv', header=True, inferSchema=True)\n",
        "    dataframes['BI_DIM_TIPO_FATURAMENTO'] = df\n",
        "    print(f'BI_DIM_TIPO_FATURAMENTO: {df.count():,} linhas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# parquet - book_atraso/dados_faturamento/\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}book_atraso/dados_faturamento/')\n",
        "    dataframes['dados_faturamento'] = df\n",
        "    print(f'dados_faturamento: {df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# parquet - bases_recarga/BI_FP_ASS_RECARGA_CMV_NOVA/\n",
        "try:\n",
        "    df = spark.read.parquet(f'{caminho_root}bases_recarga/BI_FP_ASS_RECARGA_CMV_NOVA/')\n",
        "    dataframes['BI_FP_ASS_RECARGA_CMV_NOVA'] = df\n",
        "    print(f'BI_FP_ASS_RECARGA_CMV_NOVA: {df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Erro: {str(e)[:100]}')\n",
        "\n",
        "# parquet - book_pagamento/dados_pagamento/\n",
        "import os\n",
        "caminho_pagamento = f'{caminho_root}book_pagamento/dados_pagamento/'\n",
        "\n",
        "try:\n",
        "    # Tentar carregar com wildcard\n",
        "    df = spark.read.parquet(f'{caminho_pagamento}*.parquet')\n",
        "    dataframes['dados_pagamento'] = df\n",
        "    print(f'dados_pagamento (wildcard): {df.count():,} linhas, {len(df.columns)} colunas')\n",
        "except Exception as e:\n",
        "    print(f'Wildcard falhou, tentando listar arquivos...')\n",
        "\n",
        "    # Se wildcard falhar, tentar carregar cada arquivo\n",
        "    try:\n",
        "        arquivos = os.listdir(caminho_pagamento)\n",
        "        print(f'Encontrados {len(arquivos)} arquivos:')\n",
        "\n",
        "        for arquivo in arquivos:\n",
        "            if arquivo.endswith('.parquet'):\n",
        "                print(f\"     → {arquivo}\")\n",
        "                try:\n",
        "                    caminho_completo = os.path.join(caminho_pagamento, arquivo)\n",
        "                    df = spark.read.parquet(caminho_completo)\n",
        "\n",
        "                    # Usar nome do arquivo como chave\n",
        "                    nome_tabela = arquivo.replace('.parquet', '').replace('-c000', '')\n",
        "                    if nome_tabela not in dataframes:  # Evitar duplicatas\n",
        "                        dataframes[nome_tabela] = df\n",
        "\n",
        "                    print(f'{nome_tabela}: {df.count():,} linhas')\n",
        "                except Exception as e2:\n",
        "                    print(f'Erro: {str(e2)[:50]}')\n",
        "    except Exception as e:\n",
        "        print(f'Erro ao listar diretório: {str(e)[:100]}')\n",
        "\n",
        "# Converter todos os nomes dos dataframes para minúsculas\n",
        "dataframes = {chave.lower(): df for chave, df in dataframes.items()}"
      ],
      "metadata": {
        "id": "63c7Cx5F43D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Metadados"
      ],
      "metadata": {
        "id": "SDK22a2C6U3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importar biblioteca\n",
        "from pyspark.sql.functions import col, count, when, lit\n",
        "\n",
        "\n",
        "# Lista para armazenar metadados\n",
        "metadata = []\n",
        "\n",
        "# Iterar sobre cada dataframe\n",
        "for nome_df, df in dataframes.items():\n",
        "    total_linhas = df.count()\n",
        "\n",
        "    # Analisar cada coluna\n",
        "    for coluna in df.columns:\n",
        "        tipo = df.schema[coluna].dataType\n",
        "\n",
        "        # Contar nulos\n",
        "        qt_nulos = df.filter(col(coluna).isNull()).count()\n",
        "        percent_nulos = (qt_nulos / total_linhas) * 100 if total_linhas > 0 else 0\n",
        "\n",
        "        # Cardinalidade (valores únicos)\n",
        "        cardinalidade = df.select(coluna).distinct().count()\n",
        "\n",
        "        # Adicionar à lista\n",
        "        metadata.append({\n",
        "            'nome_dataframe': nome_df,\n",
        "            'nome_variavel': coluna,\n",
        "            'tipo': str(tipo),\n",
        "            'qt_nulos': int(qt_nulos),\n",
        "            'percent_nulos': round(percent_nulos, 2),\n",
        "            'cardinalidade': int(cardinalidade)\n",
        "        })\n",
        "\n",
        "# Converter para Spark DataFrame\n",
        "df_resultado = spark.createDataFrame(metadata)\n",
        "\n",
        "# Exibir\n",
        "display(df_resultado)"
      ],
      "metadata": {
        "id": "HBkeXncV5snx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}